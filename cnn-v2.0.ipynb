{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-01-04T01:54:39.180759Z","iopub.status.busy":"2023-01-04T01:54:39.180111Z","iopub.status.idle":"2023-01-04T01:54:50.355558Z","shell.execute_reply":"2023-01-04T01:54:50.353886Z","shell.execute_reply.started":"2023-01-04T01:54:39.180645Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import os\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import random\n","\n","import gc\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","\n","from tqdm import tqdm\n","\n","from PIL import Image\n","import PIL.Image\n","PIL.Image.MAX_IMAGE_PIXELS = 9000000000000\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","\n","\n","\n","from tensorflow import keras\n","from keras import layers\n","\n","# Set up OpenSlide\n","OPENSLIDE_PATH = r'C:\\openslide-win64-20221111\\bin'\n","\n","os.add_dll_directory(OPENSLIDE_PATH)\n","import openslide\n","\n","from openslide import OpenSlide\n","# remove cap for image reading\n","os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = pow(2,40).__str__()\n","import cv2 # import after setting OPENCV_IO_MAX_IMAGE_PIXELS"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Jobs:\n","\n","+ Learn About [SHAP Values](https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137?gi=578a522915)\n","\n","+ Check Domino Data lab\n","\n","# To-do:\n","\n"," + Create a new convergence system:\n"," > The system should have a log(n) time complexity\n"," + Define the boundaries of each iamge.  Weight if it is better to have a fixed initial window size or make the intial window dependent on the image size\n"," + Edit the loss scoring method and let it score higher when you find nucleoids + redcells + fat tissue\n"," "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T01:54:50.358308Z","iopub.status.busy":"2023-01-04T01:54:50.357396Z","iopub.status.idle":"2023-01-04T01:54:50.395002Z","shell.execute_reply":"2023-01-04T01:54:50.393905Z","shell.execute_reply.started":"2023-01-04T01:54:50.358259Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv('../input/mayo-clinic-strip-ai/train.csv')\n","test = pd.read_csv('../input/mayo-clinic-strip-ai/test.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T01:55:55.235225Z","iopub.status.busy":"2023-01-04T01:55:55.233909Z","iopub.status.idle":"2023-01-04T01:55:55.270681Z","shell.execute_reply":"2023-01-04T01:55:55.269186Z","shell.execute_reply.started":"2023-01-04T01:55:55.235170Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>center_id</th>\n","      <th>patient_id</th>\n","      <th>image_num</th>\n","      <th>label</th>\n","      <th>file_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>006388_0</td>\n","      <td>11</td>\n","      <td>006388</td>\n","      <td>0</td>\n","      <td>CE</td>\n","      <td>../input/mayo-clinic-strip-ai/train/006388_0.tif</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>008e5c_0</td>\n","      <td>11</td>\n","      <td>008e5c</td>\n","      <td>0</td>\n","      <td>CE</td>\n","      <td>../input/mayo-clinic-strip-ai/train/008e5c_0.tif</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00c058_0</td>\n","      <td>11</td>\n","      <td>00c058</td>\n","      <td>0</td>\n","      <td>LAA</td>\n","      <td>../input/mayo-clinic-strip-ai/train/00c058_0.tif</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>01adc5_0</td>\n","      <td>11</td>\n","      <td>01adc5</td>\n","      <td>0</td>\n","      <td>LAA</td>\n","      <td>../input/mayo-clinic-strip-ai/train/01adc5_0.tif</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>026c97_0</td>\n","      <td>4</td>\n","      <td>026c97</td>\n","      <td>0</td>\n","      <td>CE</td>\n","      <td>../input/mayo-clinic-strip-ai/train/026c97_0.tif</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   image_id  center_id patient_id  image_num label  \\\n","0  006388_0         11     006388          0    CE   \n","1  008e5c_0         11     008e5c          0    CE   \n","2  00c058_0         11     00c058          0   LAA   \n","3  01adc5_0         11     01adc5          0   LAA   \n","4  026c97_0          4     026c97          0    CE   \n","\n","                                          file_path  \n","0  ../input/mayo-clinic-strip-ai/train/006388_0.tif  \n","1  ../input/mayo-clinic-strip-ai/train/008e5c_0.tif  \n","2  ../input/mayo-clinic-strip-ai/train/00c058_0.tif  \n","3  ../input/mayo-clinic-strip-ai/train/01adc5_0.tif  \n","4  ../input/mayo-clinic-strip-ai/train/026c97_0.tif  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# add pic file_path\n","train['file_path'] = train['image_id'].apply(lambda x: '../input/mayo-clinic-strip-ai/train/' + x + '.tif')\n","train.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T01:57:01.464006Z","iopub.status.busy":"2023-01-04T01:57:01.463429Z","iopub.status.idle":"2023-01-04T01:57:01.491537Z","shell.execute_reply":"2023-01-04T01:57:01.490082Z","shell.execute_reply.started":"2023-01-04T01:57:01.463965Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>center_id</th>\n","      <th>patient_id</th>\n","      <th>image_num</th>\n","      <th>label</th>\n","      <th>file_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>008e5c_0</td>\n","      <td>11</td>\n","      <td>008e5c</td>\n","      <td>0</td>\n","      <td>CE</td>\n","      <td>../input/mayo-clinic-strip-ai/train/008e5c_0.tif</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00c058_0</td>\n","      <td>11</td>\n","      <td>00c058</td>\n","      <td>0</td>\n","      <td>LAA</td>\n","      <td>../input/mayo-clinic-strip-ai/train/00c058_0.tif</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>09644e_0</td>\n","      <td>10</td>\n","      <td>09644e</td>\n","      <td>0</td>\n","      <td>CE</td>\n","      <td>../input/mayo-clinic-strip-ai/train/09644e_0.tif</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>09644e_1</td>\n","      <td>10</td>\n","      <td>09644e</td>\n","      <td>1</td>\n","      <td>CE</td>\n","      <td>../input/mayo-clinic-strip-ai/train/09644e_1.tif</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>09644e_2</td>\n","      <td>10</td>\n","      <td>09644e</td>\n","      <td>2</td>\n","      <td>CE</td>\n","      <td>../input/mayo-clinic-strip-ai/train/09644e_2.tif</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    image_id  center_id patient_id  image_num label  \\\n","1   008e5c_0         11     008e5c          0    CE   \n","2   00c058_0         11     00c058          0   LAA   \n","25  09644e_0         10     09644e          0    CE   \n","26  09644e_1         10     09644e          1    CE   \n","27  09644e_2         10     09644e          2    CE   \n","\n","                                           file_path  \n","1   ../input/mayo-clinic-strip-ai/train/008e5c_0.tif  \n","2   ../input/mayo-clinic-strip-ai/train/00c058_0.tif  \n","25  ../input/mayo-clinic-strip-ai/train/09644e_0.tif  \n","26  ../input/mayo-clinic-strip-ai/train/09644e_1.tif  \n","27  ../input/mayo-clinic-strip-ai/train/09644e_2.tif  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["joo = ['09644e_0', '008e5c_0', '00c058_0', '09644e_2', '09644e_1']\n","train[train['image_id'].isin(joo)]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T01:57:01.827361Z","iopub.status.busy":"2023-01-04T01:57:01.826669Z","iopub.status.idle":"2023-01-04T01:57:01.835356Z","shell.execute_reply":"2023-01-04T01:57:01.833600Z","shell.execute_reply.started":"2023-01-04T01:57:01.827300Z"},"trusted":true},"outputs":[],"source":["def periodic(val):\n","    wave = np.cos(2 * np.pi * 1/800 * val + 1/8 * np.pi) + 2\n","    return wave"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T01:57:02.610009Z","iopub.status.busy":"2023-01-04T01:57:02.608250Z","iopub.status.idle":"2023-01-04T01:57:02.622136Z","shell.execute_reply":"2023-01-04T01:57:02.620638Z","shell.execute_reply.started":"2023-01-04T01:57:02.609940Z"},"trusted":true},"outputs":[],"source":["def get_loss(region):\n","    mat = np.array(region)/255.\n","    # treshold of squares colors\n","    tresh = 1.2\n","    # treshold of the thickness of the edge:\n","    edge = 0.5\n","    # number of good choices\n","    good = 0\n","    N = mat.shape\n","    # resolution of the region (should be kept fixed)\n","    x_n, y_n = N[0], N[1]\n","    n_l = (np.ceil(x_n * 0.01), np.ceil(y_n * 0.01)) # check for 1% of the total image\n","    good2 = False\n","    # start\n","    x = 0\n","    y = 0\n","    # step \n","    step = (x_n/n_l[0], y_n/n_l[1])\n","    for i in range(int(n_l[0])):\n","        y = 0\n","        pos_x = i * step[0]\n","        for j in range(int(n_l[1])):\n","            pos_y = j * step[1]\n","            # pick different values for the coordinates\n","            pixel = mat[x, y]\n","            \n","            # calculate the cost: check if the pixel in position x, y is not white\n","            res = np.sqrt(pixel[1]**2 + pixel[2]**2) + 0.1/pixel[0]\n","            \n","            if res <= tresh:\n","                good2 = True\n","                good += 1\n","            wave = periodic(pos_y)\n","            y += int(step[1] * 0.5 * wave)\n","        wave = periodic(pos_x)\n","        x += int(step[0] * 0.5 * (wave))\n","    # return the fraction of good pixels in the region\n","    res = good/max(n_l)\n","    if res >= 0.2:\n","        return res\n","    return 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def check_cross(slide, point, window_size, tresh):\n","    m_loss = [0, 0, 0, 0]\n","    # check right\n","    point[0] += window_size[0]\n","    window = slide.read_region(point, 0, window_size)\n","    m_loss[1] = get_loss(window)\n","\n","    # check left\n","    point[0] = start_point[0]\n","    point[0] -= window_size[0]\n","    window = slide.read_region(point, 0, window_size)\n","    m_loss[0] = get_loss(window)\n","    \n","    if m_loss[1] <= tresh and m_loss[0] <= tresh:\n","        return m_loss\n","    \n","    # check bottom\n","    point[1] += window_size[1]\n","    window = slide.read_region(point, 0, window_size)\n","    m_loss[2] = get_loss(window)\n","\n","    # check top\n","    point[1] = start_point[1]\n","    point[1] -= window_size[1]\n","    window = slide.read_region(point, 0, window_size)\n","    m_loss[3] = get_loss(window)\n","    \n","    return m_loss"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-01-04T01:57:03.348679Z","iopub.status.busy":"2023-01-04T01:57:03.347281Z","iopub.status.idle":"2023-01-04T01:57:03.357838Z","shell.execute_reply":"2023-01-04T01:57:03.356069Z","shell.execute_reply.started":"2023-01-04T01:57:03.348623Z"},"trusted":true},"outputs":[],"source":["# return the ammount of blood (in fraction) in a 5k x 5k region\n","def check_window(slide, start_point, good_tresh, window_size, p_step = 1, deep = 0):\n","    point = start_point.copy()\n","    window = slide.read_region(point, 0, window_size)\n","    loss = get_loss(window)\n","    m_loss = [0, 0, 0, 0]\n","    \n","    if loss >= good_tresh:\n","        m_loss = check_cross(slide, point, window_size, good_tresh)\n","        \n","        # caso base\n","        loss = min(m_loss)\n","        if loss >= good_tresh or max(m_loss) <= good_tresh:\n","            print(f\"Retornando |{loss}|\")\n","            return loss, point\n","        \n","        if m_loss[0] <= good_tresh and m_loss[1] >= good_tresh:\n","            # go right\n","            if m_loss[2] <= good_tresh and m_loss[3] >= good_tresh:\n","                # go up\n","                point[0] += window_size[0] * p_step\n","                point[1] += window_size[1] * p_step\n","                return check_window(slide, point, good_tresh, window_size, p_step, deep + 1)\n","                \n","            elif m_loss[2] >= good_tresh and m_loss[3] <= good_tresh:\n","                # go down\n","                point[0] += window_size[0] * p_step\n","                point[1] -= window_size[1] * p_step\n","                return check_window(slide, point, good_tresh, window_size, p_step, deep + 1)\n","                \n","        elif m_loss[0] >= good_tresh and m_loss[1] <= good_tresh:\n","            # go left\n","            if m_loss[2] <= good_tresh and m_loss[3] >= good_tresh:\n","                # go up\n","                point[0] -= window_size[0] * p_step\n","                point[1] += window_size[1] * p_step\n","                return check_window(slide, point, good_tresh, window_size, p_step, deep + 1)\n","                \n","            elif m_loss[2] >= good_tresh and m_loss[3] <= good_tresh:\n","                # go down\n","                point[0] -= window_size[0] * p_step\n","                point[1] -= window_size[1] * p_step\n","                return check_window(slide, point, good_tresh, window_size, p_step, deep + 1)    \n","    # big if \n","    return loss, point\n","    "]},{"cell_type":"code","execution_count":8,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-01-04T01:57:03.837842Z","iopub.status.busy":"2023-01-04T01:57:03.837305Z","iopub.status.idle":"2023-01-04T01:57:03.850215Z","shell.execute_reply":"2023-01-04T01:57:03.848630Z","shell.execute_reply.started":"2023-01-04T01:57:03.837785Z"},"trusted":true},"outputs":[],"source":["def function(path, n_pixels = 3.6e6, window_size = (100, 100)):\n","    # get width and height of image\n","    _ = PIL.Image.open(path)\n","    width, height = _.size\n","    \n","    # Number of windows \n","    n_windows = int(n_pixels / (window_size[0] * window_size[1]))\n","    \n","    # number of pixels between each (window_size) window\n","    step = ((width - window_size[0] * n_windows) / n_windows , (height - window_size[1] * n_windows) / n_windows)\n","    start = [500, 500]\n","    \n","    for i in range(n_windows):\n","        # read and score region\n","        ## the score shoulud be based on the ammount of information present on the window \n","        ## + the ammount of info present on the surroundings\n","        \n","        start[0] += step[0]\n","        start[1] += step[1]\n","        print(\"Openning slide...\")\n","        slide = OpenSlide(path)\n","        print(\"Done\")\n","        check_window(slide, start)\n","    #  ----------------------------\n","    \n","    \n","    MAX_LOSS = 1\n","    MAX_REGION = [1,1,1]\n","    avg = 0\n","    rows = []\n","    for i in range(5):\n","#         print(f\"Window of row: {i}\")\n","        for j in range(5):\n","            max_r, m_region = check_big_window(slide, starting)\n","            if MAX_LOSS <= max_r:\n","                MAX_LOSS = max_r\n","                MAX_REGION = m_region\n","\n","            starting[0] += 5000\n","        starting[1] += 5000\n","        \n","    return MAX_LOSS, MAX_REGION"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T20:42:47.852239Z","iopub.status.busy":"2023-01-02T20:42:47.851341Z","iopub.status.idle":"2023-01-02T20:42:47.872252Z","shell.execute_reply":"2023-01-02T20:42:47.869500Z","shell.execute_reply.started":"2023-01-02T20:42:47.852166Z"},"trusted":true},"outputs":[],"source":["varios = train[train['label'] == 'CE'].iloc[0:2].file_path.to_numpy()\n","varios = np.concatenate([varios, train[train['label'] == 'LAA'].iloc[0:2].file_path.to_numpy()])\n","varios"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T20:42:49.042772Z","iopub.status.busy":"2023-01-02T20:42:49.042332Z","iopub.status.idle":"2023-01-02T20:42:52.247800Z","shell.execute_reply":"2023-01-02T20:42:52.244283Z","shell.execute_reply.started":"2023-01-02T20:42:49.042734Z"},"trusted":true},"outputs":[],"source":["%%time\n","sample_train = train[:20]\n","regions = []\n","losses = []\n","\n","varios = train[train['label'] == 'CE'].iloc[0:5].file_path.to_numpy()\n","varios = np.concatenate([varios, train[train['label'] == 'LAA'].iloc[0:5].file_path.to_numpy()])\n","\n","im = PIL.Image.open(varios[0])\n","print(im.size)\n","\n","# the default max_size of pics is 30_000 x 30_000\n","for i in varios:\n","    path = i\n","#     slide = OpenSlide(path)\n","    loss, region = function(path)\n","    print(f\"Pic {path}\")\n","    window = slide.read_region(region, 0, (800, 800))\n","    print(loss, region)\n","    plt.figure(figsize = (8, 8))\n","    plt.imshow(window)\n","    plt.show()\n","    size = (800, 800)\n","    x = 500\n","    y = 500\n","    print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# directly process the data\n","def process(path):\n","    size  = (800, 800)\n","    slide = OpenSlide(path)\n","    loss, region = function(slide)\n","    image = slide.read_region(region, 0, size)\n","    plt.figure(figsize = (8, 8))\n","    plt.imshow(image)\n","    plt.show()\n","    image = tf.image.resize(image, (512, 512))\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["X = []\n","for path in tqdm(train['file_path']):\n","    print(f\"Processing {path}\")\n","    image = process(path)\n","    X.append(image)\n","    name = path.split(\"/\")[-1]\n","    tf.keras.utils.save_img(f\"/kaggle/working/processed/{name}\", image)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["toy = train.copy()\n","files = []\n","paths = os.scandir(\"/kaggle/working/processed\")\n","for part in paths:\n","    if part.is_file() and (part.name.find(\"tif\") > 0):\n","        files.append(part.name)\n","        \n","\n","toy = toy.set_index('image_id')\n","\n","for file in files:\n","    print()\n","    name = file.split(\".\")[0]\n","    label = toy.loc[name, 'label']\n","    print(name, label)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def step_decay(epoch):\n","    initial_lrate = 0.001\n","    drop = 0.5\n","    epochs_drop = 10.0\n","    lrate = initial_lrate * drop ** np.floor((epoch)/epochs_drop)\n","    return lrate\n","\n","l_rate = LearningRateScheduler(step_decay)\n","earstop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["shapes = (512, 512, 4)\n","\n","input_layer = layers.Input(name = 'input', shape = shapes)\n","conv_1 = layers.Conv2D(filters=32, kernel_size = (3,3), strides =2, padding = 'same', activation = 'relu', input_shape = shapes, name = 'conv_1')(input_layer)\n","conv_2 = layers.Conv2D(filters=64, kernel_size = (3,3), strides =2, padding = 'same', activation = 'relu', name = 'conv_2')(conv_1)\n","conv_3 = layers.Conv2D(filters=32, kernel_size = (3,3), strides =2, padding = 'same', activation = 'relu', name = 'conv_3')(conv_2)\n","flat = layers.Flatten()(conv_3)\n","h1 = layers.Dense(128, activation = 'relu', name = 'h1')(flat)\n","drop = layers.Dropout(0.25)(h1)\n","output = layers.Dense(1, activation = 'sigmoid', name = 'prediction')(h1)\n","\n","model = tf.keras.Model(input_layer, output)\n","\n","model.compile(optimizer = 'adam', \n","                loss = tf.keras.losses.BinaryCrossentropy(), \n","                 metrics = ['accuracy', 'mse', 'mape']\n","             )\n","\n","tf.keras.utils.plot_model(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vocab = train['label'].unique().tolist()\n","Y = train['label'].apply(lambda x: vocab.index(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ret = pd.DataFrame((X, Y)).transpose()\n","ret.columns = ['input', 'label']\n","ret.to_csv('processed.csv', index = False, header = None)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","dataset = pd.read_csv('processed.csv')\n","train, test = train_test_split(dataset, test_size = 0.2)\n","train, val = train_test_split(dataset, test_size = 0.2)\n","\n","train = tf.data.Dataset.from_tensor_slices((dict(train))).batch(BATCH_SIZE)\n","val = tf.data.Dataset.from_tensor_slices((dict(val))).batch(BATCH_SIZE)\n","test = tf.data.Dataset.from_tensor_slices((dict(test))).batch(1)\n","dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in train.take(1):\n","    print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["EPOCHS = 20\n","STEPS_X_EPOCH = 2\n","history = model.fit(train, validation_data = val, epochs = EPOCHS, callbacks = [l_rate, earstop])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pred = model.predict(test_df)\n","pred"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y = y_test.to_numpy()\n","y_ = []\n","for i in pred:\n","    i_r = i.round()\n","    if i_r >= 1:\n","        y_.append(1)\n","    else:\n","        y_.append(0)\n","\n","print(f\" Accuracy: {1 - sum(abs(y - y_))/len(y_)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for __ in test_df.take(1):\n","    print(__)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}}},"nbformat":4,"nbformat_minor":4}

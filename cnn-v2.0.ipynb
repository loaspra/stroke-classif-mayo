{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport random\n\nimport gc\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\n\nfrom PIL import Image\nimport PIL.Image\nPIL.Image.MAX_IMAGE_PIXELS = 9000000000000\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom tensorflow.keras.layers import GlobalMaxPooling2D\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, Callback\n\n\nimport openslide\nfrom openslide import OpenSlide\n\n# remove cap for image reading\nos.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = pow(2,40).__str__()\nimport cv2 # import after setting OPENCV_IO_MAX_IMAGE_PIXELS","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-04T01:54:39.180111Z","iopub.execute_input":"2023-01-04T01:54:39.180759Z","iopub.status.idle":"2023-01-04T01:54:50.355558Z","shell.execute_reply.started":"2023-01-04T01:54:39.180645Z","shell.execute_reply":"2023-01-04T01:54:50.353886Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Jobs:\n\n+ Learn https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137?gi=578a522915\n\n+ Check Domino Data lab\n\n# To-do:\n\n + Create a new convergence system:\n > The system should have a log(n) time complexity\n + Define the boundaries of each iamge.  Weight if it is better to have a fixed initial window size or make the intial window dependent on the image size\n + Edit the loss scoring method and let it score higher when you find nucleoids + redcells + fat tissue\n ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/mayo-clinic-strip-ai/train.csv')\ntest = pd.read_csv('../input/mayo-clinic-strip-ai/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-04T01:54:50.357396Z","iopub.execute_input":"2023-01-04T01:54:50.358308Z","iopub.status.idle":"2023-01-04T01:54:50.395002Z","shell.execute_reply.started":"2023-01-04T01:54:50.358259Z","shell.execute_reply":"2023-01-04T01:54:50.393905Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# add pic file_path\ntrain['file_path'] = train['image_id'].apply(lambda x: '../input/mayo-clinic-strip-ai/train/' + x + '.tif')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-04T01:55:55.233909Z","iopub.execute_input":"2023-01-04T01:55:55.235225Z","iopub.status.idle":"2023-01-04T01:55:55.270681Z","shell.execute_reply.started":"2023-01-04T01:55:55.235170Z","shell.execute_reply":"2023-01-04T01:55:55.269186Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   image_id  center_id patient_id  image_num label  \\\n0  006388_0         11     006388          0    CE   \n1  008e5c_0         11     008e5c          0    CE   \n2  00c058_0         11     00c058          0   LAA   \n3  01adc5_0         11     01adc5          0   LAA   \n4  026c97_0          4     026c97          0    CE   \n\n                                          file_path  \n0  ../input/mayo-clinic-strip-ai/train/006388_0.tif  \n1  ../input/mayo-clinic-strip-ai/train/008e5c_0.tif  \n2  ../input/mayo-clinic-strip-ai/train/00c058_0.tif  \n3  ../input/mayo-clinic-strip-ai/train/01adc5_0.tif  \n4  ../input/mayo-clinic-strip-ai/train/026c97_0.tif  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>center_id</th>\n      <th>patient_id</th>\n      <th>image_num</th>\n      <th>label</th>\n      <th>file_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>006388_0</td>\n      <td>11</td>\n      <td>006388</td>\n      <td>0</td>\n      <td>CE</td>\n      <td>../input/mayo-clinic-strip-ai/train/006388_0.tif</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>008e5c_0</td>\n      <td>11</td>\n      <td>008e5c</td>\n      <td>0</td>\n      <td>CE</td>\n      <td>../input/mayo-clinic-strip-ai/train/008e5c_0.tif</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00c058_0</td>\n      <td>11</td>\n      <td>00c058</td>\n      <td>0</td>\n      <td>LAA</td>\n      <td>../input/mayo-clinic-strip-ai/train/00c058_0.tif</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01adc5_0</td>\n      <td>11</td>\n      <td>01adc5</td>\n      <td>0</td>\n      <td>LAA</td>\n      <td>../input/mayo-clinic-strip-ai/train/01adc5_0.tif</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>026c97_0</td>\n      <td>4</td>\n      <td>026c97</td>\n      <td>0</td>\n      <td>CE</td>\n      <td>../input/mayo-clinic-strip-ai/train/026c97_0.tif</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"joo = ['09644e_0', '008e5c_0', '00c058_0', '09644e_2', '09644e_1']\ntrain[train['image_id'].isin(joo)]","metadata":{"execution":{"iopub.status.busy":"2023-01-04T01:57:01.463429Z","iopub.execute_input":"2023-01-04T01:57:01.464006Z","iopub.status.idle":"2023-01-04T01:57:01.491537Z","shell.execute_reply.started":"2023-01-04T01:57:01.463965Z","shell.execute_reply":"2023-01-04T01:57:01.490082Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"    image_id  center_id patient_id  image_num label  \\\n1   008e5c_0         11     008e5c          0    CE   \n2   00c058_0         11     00c058          0   LAA   \n25  09644e_0         10     09644e          0    CE   \n26  09644e_1         10     09644e          1    CE   \n27  09644e_2         10     09644e          2    CE   \n\n                                           file_path  \n1   ../input/mayo-clinic-strip-ai/train/008e5c_0.tif  \n2   ../input/mayo-clinic-strip-ai/train/00c058_0.tif  \n25  ../input/mayo-clinic-strip-ai/train/09644e_0.tif  \n26  ../input/mayo-clinic-strip-ai/train/09644e_1.tif  \n27  ../input/mayo-clinic-strip-ai/train/09644e_2.tif  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>center_id</th>\n      <th>patient_id</th>\n      <th>image_num</th>\n      <th>label</th>\n      <th>file_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>008e5c_0</td>\n      <td>11</td>\n      <td>008e5c</td>\n      <td>0</td>\n      <td>CE</td>\n      <td>../input/mayo-clinic-strip-ai/train/008e5c_0.tif</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00c058_0</td>\n      <td>11</td>\n      <td>00c058</td>\n      <td>0</td>\n      <td>LAA</td>\n      <td>../input/mayo-clinic-strip-ai/train/00c058_0.tif</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>09644e_0</td>\n      <td>10</td>\n      <td>09644e</td>\n      <td>0</td>\n      <td>CE</td>\n      <td>../input/mayo-clinic-strip-ai/train/09644e_0.tif</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>09644e_1</td>\n      <td>10</td>\n      <td>09644e</td>\n      <td>1</td>\n      <td>CE</td>\n      <td>../input/mayo-clinic-strip-ai/train/09644e_1.tif</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>09644e_2</td>\n      <td>10</td>\n      <td>09644e</td>\n      <td>2</td>\n      <td>CE</td>\n      <td>../input/mayo-clinic-strip-ai/train/09644e_2.tif</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def periodic(val):\n    wave = np.cos(2 * np.pi * 1/800 * val + 1/8 * np.pi) + 2\n    return wave","metadata":{"execution":{"iopub.status.busy":"2023-01-04T01:57:01.826669Z","iopub.execute_input":"2023-01-04T01:57:01.827361Z","iopub.status.idle":"2023-01-04T01:57:01.835356Z","shell.execute_reply.started":"2023-01-04T01:57:01.827300Z","shell.execute_reply":"2023-01-04T01:57:01.833600Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_loss(region):\n    mat = np.array(region)/255.\n    # treshold of squares colors\n    tresh = 1.2\n    # treshold of the thickness of the edge:\n    edge = 0.5\n    # number of good choices\n    good = 0\n    N = mat.shape\n    # resolution of the region (should be kept fixed)\n    x_n, y_n = N[0], N[1]\n    n_l = (np.ceil(x_n * 0.01), np.ceil(y_n * 0.01)) # check for 1% of the total image\n    good2 = False\n    # start\n    x = 0\n    y = 0\n    # step \n    step = (x_n/n_l[0], y_n/n_l[1])\n    for i in range(int(n_l[0])):\n        y = 0\n        pos_x = i * step[0]\n        for j in range(int(n_l[1])):\n            pos_y = j * step[1]\n            # pick different values for the coordinates\n            pixel = mat[x, y]\n            \n            # calculate the cost: check if the pixel in position x, y is not white\n            res = np.sqrt(pixel[1]**2 + pixel[2]**2) + 0.1/pixel[0]\n            \n            if res <= tresh:\n                good2 = True\n                good += 1\n            wave = periodic(pos_y)\n            y += int(step[1] * 0.5 * wave)\n        wave = periodic(pos_x)\n        x += int(step[0] * 0.5 * (wave))\n    # return the fraction of good pixels in the region\n    res = good/max(n_l)\n    if res >= 0.2:\n        return res\n    return 0","metadata":{"execution":{"iopub.status.busy":"2023-01-04T01:57:02.608250Z","iopub.execute_input":"2023-01-04T01:57:02.610009Z","iopub.status.idle":"2023-01-04T01:57:02.622136Z","shell.execute_reply.started":"2023-01-04T01:57:02.609940Z","shell.execute_reply":"2023-01-04T01:57:02.620638Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def check_cross(slide, point, window_size, tresh):\n    m_loss = [0, 0, 0, 0]\n    # check right\n    point[0] += window_size[0]\n    window = slide.read_region(point, 0, window_size)\n    m_loss[1] = get_loss(window)\n\n    # check left\n    point[0] = start_point[0]\n    point[0] -= window_size[0]\n    window = slide.read_region(point, 0, window_size)\n    m_loss[0] = get_loss(window)\n    \n    if m_loss[1] <= tresh and m_loss[0] <= tresh:\n        return m_loss\n    \n    # check bottom\n    point[1] += window_size[1]\n    window = slide.read_region(point, 0, window_size)\n    m_loss[2] = get_loss(window)\n\n    # check top\n    point[1] = start_point[1]\n    point[1] -= window_size[1]\n    window = slide.read_region(point, 0, window_size)\n    m_loss[3] = get_loss(window)\n    \n    return m_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# return the ammount of blood (in fraction) in a 5k x 5k region\ndef check_window(slide, start_point, good_tresh, window_size, p_step = 1, deep = 0):\n    point = start_point.copy()\n    window = slide.read_region(point, 0, window_size)\n    loss = get_loss(window)\n    m_loss = [0, 0, 0, 0]\n    \n    if loss >= good_tresh:\n        m_loss = check_cross(slide, point, window_size, good_tresh)\n        \n        # caso base\n        loss = min(m_loss)\n        if loss >= good_tresh or max(m_loss) <= good_tresh:\n            print(f\"Retornando |{loss}|\")\n            return loss, point\n        \n        if m_loss[0] <= good_tresh and m_loss[1] >= good_tresh:\n            # go right\n            if m_loss[2] <= good_tresh and m_loss[3] >= good_tresh:\n                # go up\n                point[0] += window_size[0] * p_step\n                point[1] += window_size[1] * p_step\n                return check_window(slide, point, good_tresh, window_size, p_step, deep + 1)\n                \n            elif m_loss[2] >= good_tresh and m_loss[3] <= good_tresh:\n                # go down\n                point[0] += window_size[0] * p_step\n                point[1] -= window_size[1] * p_step\n                return check_window(slide, point, good_tresh, window_size, p_step, deep + 1)\n                \n        elif m_loss[0] >= good_tresh and m_loss[1] <= good_tresh:\n            # go left\n            if m_loss[2] <= good_tresh and m_loss[3] >= good_tresh:\n                # go up\n                point[0] -= window_size[0] * p_step\n                point[1] += window_size[1] * p_step\n                return check_window(slide, point, good_tresh, window_size, p_step, deep + 1)\n                \n            elif m_loss[2] >= good_tresh and m_loss[3] <= good_tresh:\n                # go down\n                point[0] -= window_size[0] * p_step\n                point[1] -= window_size[1] * p_step\n                return check_window(slide, point, good_tresh, window_size, p_step, deep + 1)    \n    # big if \n    return loss, point\n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-04T01:57:03.347281Z","iopub.execute_input":"2023-01-04T01:57:03.348679Z","iopub.status.idle":"2023-01-04T01:57:03.357838Z","shell.execute_reply.started":"2023-01-04T01:57:03.348623Z","shell.execute_reply":"2023-01-04T01:57:03.356069Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def function(path, n_pixels = 3.6e6, window_size = (100, 100)):\n    # get width and height of image\n    _ = PIL.Image.open(path)\n    width, height = _.size\n    \n    # Number of windows \n    n_windows = int(n_pixels / (window_size[0] * window_size[1]))\n    \n    # number of pixels between each (window_size) window\n    step = ((width - window_size[0] * n_windows) / n_windows , (height - window_size[1] * n_windows) / n_windows)\n    start = [500, 500]\n    \n    for i in range(n_windows):\n        # read and score region\n        ## the score shoulud be based on the ammount of information present on the window \n        ## + the ammount of info present on the surroundings\n        \n        start[0] += step[0]\n        start[1] += step[1]\n        print(\"Openning slide...\")\n        slide = OpenSlide(path)\n        print(\"Done\")\n        check_window(slide, start)\n    #  ----------------------------\n    \n    \n    MAX_LOSS = 1\n    MAX_REGION = [1,1,1]\n    avg = 0\n    rows = []\n    for i in range(5):\n#         print(f\"Window of row: {i}\")\n        for j in range(5):\n            max_r, m_region = check_big_window(slide, starting)\n            if MAX_LOSS <= max_r:\n                MAX_LOSS = max_r\n                MAX_REGION = m_region\n\n            starting[0] += 5000\n        starting[1] += 5000\n        \n    return MAX_LOSS, MAX_REGION","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-04T01:57:03.837305Z","iopub.execute_input":"2023-01-04T01:57:03.837842Z","iopub.status.idle":"2023-01-04T01:57:03.850215Z","shell.execute_reply.started":"2023-01-04T01:57:03.837785Z","shell.execute_reply":"2023-01-04T01:57:03.848630Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"varios = train[train['label'] == 'CE'].iloc[0:2].file_path.to_numpy()\nvarios = np.concatenate([varios, train[train['label'] == 'LAA'].iloc[0:2].file_path.to_numpy()])\nvarios","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:42:47.851341Z","iopub.execute_input":"2023-01-02T20:42:47.852239Z","iopub.status.idle":"2023-01-02T20:42:47.872252Z","shell.execute_reply.started":"2023-01-02T20:42:47.852166Z","shell.execute_reply":"2023-01-02T20:42:47.869500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsample_train = train[:20]\nregions = []\nlosses = []\n\nvarios = train[train['label'] == 'CE'].iloc[0:5].file_path.to_numpy()\nvarios = np.concatenate([varios, train[train['label'] == 'LAA'].iloc[0:5].file_path.to_numpy()])\n\nim = PIL.Image.open(varios[0])\nprint(im.size)\n\n# the default max_size of pics is 30_000 x 30_000\nfor i in varios:\n    path = i\n#     slide = OpenSlide(path)\n    loss, region = function(path)\n    print(f\"Pic {path}\")\n    window = slide.read_region(region, 0, (800, 800))\n    print(loss, region)\n    plt.figure(figsize = (8, 8))\n    plt.imshow(window)\n    plt.show()\n    size = (800, 800)\n    x = 500\n    y = 500\n    print(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:42:49.042332Z","iopub.execute_input":"2023-01-02T20:42:49.042772Z","iopub.status.idle":"2023-01-02T20:42:52.247800Z","shell.execute_reply.started":"2023-01-02T20:42:49.042734Z","shell.execute_reply":"2023-01-02T20:42:52.244283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# directly process the data\ndef process(path):\n    size  = (800, 800)\n    slide = OpenSlide(path)\n    loss, region = function(slide)\n    image = slide.read_region(region, 0, size)\n    plt.figure(figsize = (8, 8))\n    plt.imshow(image)\n    plt.show()\n    image = tf.image.resize(image, (512, 512))\n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = []\nfor path in tqdm(train['file_path']):\n    print(f\"Processing {path}\")\n    image = process(path)\n    X.append(image)\n    name = path.split(\"/\")[-1]\n    tf.keras.utils.save_img(f\"/kaggle/working/processed/{name}\", image)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toy = train.copy()\nfiles = []\npaths = os.scandir(\"/kaggle/working/processed\")\nfor part in paths:\n    if part.is_file() and (part.name.find(\"tif\") > 0):\n        files.append(part.name)\n        \n\ntoy = toy.set_index('image_id')\n\nfor file in files:\n    print()\n    name = file.split(\".\")[0]\n    label = toy.loc[name, 'label']\n    print(name, label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def step_decay(epoch):\n    initial_lrate = 0.001\n    drop = 0.5\n    epochs_drop = 10.0\n    lrate = initial_lrate * drop ** np.floor((epoch)/epochs_drop)\n    return lrate\n\nl_rate = LearningRateScheduler(step_decay)\nearstop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shapes = (512, 512, 4)\n\ninput_layer = layers.Input(name = 'input', shape = shapes)\nconv_1 = layers.Conv2D(filters=32, kernel_size = (3,3), strides =2, padding = 'same', activation = 'relu', input_shape = shapes, name = 'conv_1')(input_layer)\nconv_2 = layers.Conv2D(filters=64, kernel_size = (3,3), strides =2, padding = 'same', activation = 'relu', name = 'conv_2')(conv_1)\nconv_3 = layers.Conv2D(filters=32, kernel_size = (3,3), strides =2, padding = 'same', activation = 'relu', name = 'conv_3')(conv_2)\nflat = layers.Flatten()(conv_3)\nh1 = layers.Dense(128, activation = 'relu', name = 'h1')(flat)\ndrop = layers.Dropout(0.25)(h1)\noutput = layers.Dense(1, activation = 'sigmoid', name = 'prediction')(h1)\n\nmodel = tf.keras.Model(input_layer, output)\n\nmodel.compile(optimizer = 'adam', \n                loss = tf.keras.losses.BinaryCrossentropy(), \n                 metrics = ['accuracy', 'mse', 'mape']\n             )\n\ntf.keras.utils.plot_model(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = train['label'].unique().tolist()\nY = train['label'].apply(lambda x: vocab.index(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ret = pd.DataFrame((X, Y)).transpose()\nret.columns = ['input', 'label']\nret.to_csv('processed.csv', index = False, header = None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndataset = pd.read_csv('processed.csv')\ntrain, test = train_test_split(dataset, test_size = 0.2)\ntrain, val = train_test_split(dataset, test_size = 0.2)\n\ntrain = tf.data.Dataset.from_tensor_slices((dict(train))).batch(BATCH_SIZE)\nval = tf.data.Dataset.from_tensor_slices((dict(val))).batch(BATCH_SIZE)\ntest = tf.data.Dataset.from_tensor_slices((dict(test))).batch(1)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train.take(1):\n    print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\nSTEPS_X_EPOCH = 2\nhistory = model.fit(train, validation_data = val, epochs = EPOCHS, callbacks = [l_rate, earstop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(test_df)\npred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = y_test.to_numpy()\ny_ = []\nfor i in pred:\n    i_r = i.round()\n    if i_r >= 1:\n        y_.append(1)\n    else:\n        y_.append(0)\n\nprint(f\" Accuracy: {1 - sum(abs(y - y_))/len(y_)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for __ in test_df.take(1):\n    print(__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
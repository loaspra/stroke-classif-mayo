{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport random\n\nimport gc\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\n\nfrom PIL import Image\nimport PIL.Image\nPIL.Image.MAX_IMAGE_PIXELS = 9000000000000\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom tensorflow.keras.layers import GlobalMaxPooling2D\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, Callback\n\n\nimport openslide\nfrom openslide import OpenSlide\n\n# remove cap for image reading\nos.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = pow(2,40).__str__()\nimport cv2 # import after setting OPENCV_IO_MAX_IMAGE_PIXELS","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-02T20:42:37.195477Z","iopub.execute_input":"2023-01-02T20:42:37.195958Z","iopub.status.idle":"2023-01-02T20:42:47.669589Z","shell.execute_reply.started":"2023-01-02T20:42:37.195859Z","shell.execute_reply":"2023-01-02T20:42:47.668210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Jobs:\n\n+ Learn !(shap values analysis)[https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137?gi=578a522915]\n\n+ Check Domino Data lab\n\n# To-do:\n\n + Create a new convergence system:\n > The system should have a log(n) time complexity\n + Define the boundaries of each iamge.  Weight if it is better to have a fixed initial window size or make the intial window dependent on the image size\n + Edit the loss scoring method and let it score higher when you find nucleoids + redcells + fat tissue\n ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/mayo-clinic-strip-ai/train.csv')\ntest = pd.read_csv('../input/mayo-clinic-strip-ai/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:42:47.671834Z","iopub.execute_input":"2023-01-02T20:42:47.672642Z","iopub.status.idle":"2023-01-02T20:42:47.705840Z","shell.execute_reply.started":"2023-01-02T20:42:47.672600Z","shell.execute_reply":"2023-01-02T20:42:47.704713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add pic file_path\ntrain['file_path'] = train['image_id'].apply(lambda x: '../input/mayo-clinic-strip-ai/train/' + x + '.tif')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:42:47.707707Z","iopub.execute_input":"2023-01-02T20:42:47.708482Z","iopub.status.idle":"2023-01-02T20:42:47.753097Z","shell.execute_reply.started":"2023-01-02T20:42:47.708430Z","shell.execute_reply":"2023-01-02T20:42:47.751673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joo = ['09644e_0', '008e5c_0', '00c058_0', '09644e_2', '09644e_1']\ntrain[train['image_id'].isin(joo)]","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:42:47.756312Z","iopub.execute_input":"2023-01-02T20:42:47.757529Z","iopub.status.idle":"2023-01-02T20:42:47.783733Z","shell.execute_reply.started":"2023-01-02T20:42:47.757485Z","shell.execute_reply":"2023-01-02T20:42:47.782382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def periodic(val):\n    wave = np.cos(2 * np.pi * 1/800 * val + 1/8 * np.pi) + 2\n    return wave","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:42:47.785736Z","iopub.execute_input":"2023-01-02T20:42:47.786259Z","iopub.status.idle":"2023-01-02T20:42:47.795819Z","shell.execute_reply.started":"2023-01-02T20:42:47.786187Z","shell.execute_reply":"2023-01-02T20:42:47.794319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_loss(region):\n    mat = np.array(region)/255.\n    # treshold of squares colors\n    tresh = 1.2\n    # treshold of the thickness of the edge:\n    edge = 0.5\n    # number of good choices\n    good = 0\n    N = mat.shape\n    # resolution of the region (should be kept fixed)\n    x_n, y_n = N[0], N[1]\n    n_l = (np.ceil(x_n * 0.01), np.ceil(y_n * 0.01)) # check for 1% of the total image\n    good2 = False\n    # start\n    x = 0\n    y = 0\n    # step \n    step = (x_n/n_l[0], y_n/n_l[1])\n    for i in range(int(n_l[0])):\n        y = 0\n        pos_x = i * step[0]\n        for j in range(int(n_l[1])):\n            pos_y = j * step[1]\n            # pick different values for the coordinates\n            pixel = mat[x, y]\n            \n            # calculate the cost: check if the pixel in position x, y is not white\n            res = np.sqrt(pixel[1]**2 + pixel[2]**2) + 0.1/pixel[0]\n            \n            if res <= tresh:\n                good2 = True\n                good += 1\n            wave = periodic(pos_y)\n            y += int(step[1] * 0.5 * wave)\n        wave = periodic(pos_x)\n        x += int(step[0] * 0.5 * (wave))\n    # return the fraction of good pixels in the region\n    res = good/max(n_l)\n    if res >= 0.2:\n        return res\n    return 0","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:42:47.797782Z","iopub.execute_input":"2023-01-02T20:42:47.799029Z","iopub.status.idle":"2023-01-02T20:42:47.813582Z","shell.execute_reply.started":"2023-01-02T20:42:47.798966Z","shell.execute_reply":"2023-01-02T20:42:47.812285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# return the ammount of blood (in fraction) in a 5k x 5k region\ndef check_big_window(slide, start_point):\n    # fraction of the region with dark clothes of blood\n    point = start_point.copy()\n    loss = 0\n    n = 5\n    \n    max_r = 1\n    m_region = [1, 2, 3]\n    for i in range(n):\n        point[0] = start_point[0]\n        for j in range(n):\n            region = (point[0] + 100, point[1] + 100)\n            window = slide.read_region(region, 0, (800, 800))\n            loss = get_loss(window)\n            point[0] += 1000\n            if loss >= max_r:\n                max_r = loss\n                m_region = region\n                \n        point[1] += 1000\n    \n    return max_r, m_region","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:42:47.815628Z","iopub.execute_input":"2023-01-02T20:42:47.816195Z","iopub.status.idle":"2023-01-02T20:42:47.832621Z","shell.execute_reply.started":"2023-01-02T20:42:47.816146Z","shell.execute_reply":"2023-01-02T20:42:47.831197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def function(path, n_pixels = 3.6e6, window_size = (100, 100)):\n    # get width and height of image\n    _ = PIL.Image.open(path)\n    width, height = _.size\n    \n    # Number of windows \n    n_windows = int(n_pixels / (window_size[0] * window_size[1]))\n    \n    # number of pixels between each (window_size) window\n    step = ((width - window_size[0] * n_windows) / n_windows , (height - window_size[1] * n_windows) / n_windows)\n    start = [500, 500]\n    \n    for i in range(n_windows):\n        # read and score region\n        #### the score shoulud be based on the ammount of information present on the window \n        #### + the ammount of info present on the surroundings\n        \n        start[0] += step[0]\n        start[1] += step[1]\n        \n    \n    #  ----------------------------\n    \n    \n    MAX_LOSS = 1\n    MAX_REGION = [1,1,1]\n    avg = 0\n    rows = []\n    for i in range(5):\n#         print(f\"Window of row: {i}\")\n        for j in range(5):\n            max_r, m_region = check_big_window(slide, starting)\n            if MAX_LOSS <= max_r:\n                MAX_LOSS = max_r\n                MAX_REGION = m_region\n\n            starting[0] += 5000\n        starting[1] += 5000\n        \n    return MAX_LOSS, MAX_REGION","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-02T20:42:47.834522Z","iopub.execute_input":"2023-01-02T20:42:47.835285Z","iopub.status.idle":"2023-01-02T20:42:47.848527Z","shell.execute_reply.started":"2023-01-02T20:42:47.835211Z","shell.execute_reply":"2023-01-02T20:42:47.847059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"varios = train[train['label'] == 'CE'].iloc[0:2].file_path.to_numpy()\nvarios = np.concatenate([varios, train[train['label'] == 'LAA'].iloc[0:2].file_path.to_numpy()])\nvarios","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:42:47.851341Z","iopub.execute_input":"2023-01-02T20:42:47.852239Z","iopub.status.idle":"2023-01-02T20:42:47.872252Z","shell.execute_reply.started":"2023-01-02T20:42:47.852166Z","shell.execute_reply":"2023-01-02T20:42:47.869500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsample_train = train[:20]\nregions = []\nlosses = []\n\nvarios = train[train['label'] == 'CE'].iloc[0:5].file_path.to_numpy()\nvarios = np.concatenate([varios, train[train['label'] == 'LAA'].iloc[0:5].file_path.to_numpy()])\n\nim = PIL.Image.open(varios[0])\nprint(im.size)\n\n# the default max_size of pics is 30_000 x 30_000\nfor i in varios:\n    path = i\n#     slide = OpenSlide(path)\n    loss, region = function(path)\n    print(f\"Pic {path}\")\n    window = slide.read_region(region, 0, (800, 800))\n    print(loss, region)\n    plt.figure(figsize = (8, 8))\n    plt.imshow(window)\n    plt.show()\n    size = (800, 800)\n    x = 500\n    y = 500\n    print(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2023-01-02T20:42:49.042332Z","iopub.execute_input":"2023-01-02T20:42:49.042772Z","iopub.status.idle":"2023-01-02T20:42:52.247800Z","shell.execute_reply.started":"2023-01-02T20:42:49.042734Z","shell.execute_reply":"2023-01-02T20:42:52.244283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# directly process the data\ndef process(path):\n    size  = (800, 800)\n    slide = OpenSlide(path)\n    loss, region = function(slide)\n    image = slide.read_region(region, 0, size)\n    plt.figure(figsize = (8, 8))\n    plt.imshow(image)\n    plt.show()\n    image = tf.image.resize(image, (512, 512))\n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = []\nfor path in tqdm(train['file_path']):\n    print(f\"Processing {path}\")\n    image = process(path)\n    X.append(image)\n    name = path.split(\"/\")[-1]\n    tf.keras.utils.save_img(f\"/kaggle/working/processed/{name}\", image)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toy = train.copy()\nfiles = []\npaths = os.scandir(\"/kaggle/working/processed\")\nfor part in paths:\n    if part.is_file() and (part.name.find(\"tif\") > 0):\n        files.append(part.name)\n        \n\ntoy = toy.set_index('image_id')\n\nfor file in files:\n    print()\n    name = file.split(\".\")[0]\n    label = toy.loc[name, 'label']\n    print(name, label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def step_decay(epoch):\n    initial_lrate = 0.001\n    drop = 0.5\n    epochs_drop = 10.0\n    lrate = initial_lrate * drop ** np.floor((epoch)/epochs_drop)\n    return lrate\n\nl_rate = LearningRateScheduler(step_decay)\nearstop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shapes = (512, 512, 4)\n\ninput_layer = layers.Input(name = 'input', shape = shapes)\nconv_1 = layers.Conv2D(filters=32, kernel_size = (3,3), strides =2, padding = 'same', activation = 'relu', input_shape = shapes, name = 'conv_1')(input_layer)\nconv_2 = layers.Conv2D(filters=64, kernel_size = (3,3), strides =2, padding = 'same', activation = 'relu', name = 'conv_2')(conv_1)\nconv_3 = layers.Conv2D(filters=32, kernel_size = (3,3), strides =2, padding = 'same', activation = 'relu', name = 'conv_3')(conv_2)\nflat = layers.Flatten()(conv_3)\nh1 = layers.Dense(128, activation = 'relu', name = 'h1')(flat)\ndrop = layers.Dropout(0.25)(h1)\noutput = layers.Dense(1, activation = 'sigmoid', name = 'prediction')(h1)\n\nmodel = tf.keras.Model(input_layer, output)\n\nmodel.compile(optimizer = 'adam', \n                loss = tf.keras.losses.BinaryCrossentropy(), \n                 metrics = ['accuracy', 'mse', 'mape']\n             )\n\ntf.keras.utils.plot_model(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = train['label'].unique().tolist()\nY = train['label'].apply(lambda x: vocab.index(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ret = pd.DataFrame((X, Y)).transpose()\nret.columns = ['input', 'label']\nret.to_csv('processed.csv', index = False, header = None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndataset = pd.read_csv('processed.csv')\ntrain, test = train_test_split(dataset, test_size = 0.2)\ntrain, val = train_test_split(dataset, test_size = 0.2)\n\ntrain = tf.data.Dataset.from_tensor_slices((dict(train))).batch(BATCH_SIZE)\nval = tf.data.Dataset.from_tensor_slices((dict(val))).batch(BATCH_SIZE)\ntest = tf.data.Dataset.from_tensor_slices((dict(test))).batch(1)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train.take(1):\n    print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\nSTEPS_X_EPOCH = 2\nhistory = model.fit(train, validation_data = val, epochs = EPOCHS, callbacks = [l_rate, earstop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(test_df)\npred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = y_test.to_numpy()\ny_ = []\nfor i in pred:\n    i_r = i.round()\n    if i_r >= 1:\n        y_.append(1)\n    else:\n        y_.append(0)\n\nprint(f\" Accuracy: {1 - sum(abs(y - y_))/len(y_)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for __ in test_df.take(1):\n    print(__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}